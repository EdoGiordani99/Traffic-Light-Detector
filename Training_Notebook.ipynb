{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Yn3tTBoOjpcm",
        "WjGpyVFmrReG"
      ],
      "authorship_tag": "ABX9TyOxYSUQl96fMT2qtxiCoybH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdoGiordani99/Traffic-Light-Detector/blob/main/Training_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object Detection Training Notebook (from scratch)\n",
        "<h2>Vision & Perception Project 2021/2022</h2>\n",
        "<h3>Student: Edoardo Giordani</h3>\n",
        "<h3>E-mail: giordani.2020434@studenti.uniroma1.it</h3>\n",
        "\n",
        "This notebook was used to train my from scratch implementation of a Object Detection Network. I took inspiration from the Faster RCNN architecture which turned out to be one of the most popular approach of solving the object detection problem. "
      ],
      "metadata": {
        "id": "kdcJxOEQkSsi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mounting Drive\n",
        "This will allow you to save the plots and models in your drive"
      ],
      "metadata": {
        "id": "gu8Mg_83vVdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/V&P/TLD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJSgizgBnPBp",
        "outputId": "f5bf5806-ccc1-45e6-9f64-5d35929e6c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/V&P/TLD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies & Import Libraries\n",
        "As first thing, we install the dependencies and we import the libraries."
      ],
      "metadata": {
        "id": "niqHqSz6jiXd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMy7eMAIHX7t"
      },
      "outputs": [],
      "source": [
        "# REQUIREMENTS\n",
        "!pip install roboflow\n",
        "!pip install albumentations==0.4.6\n",
        "!pip install pytest-shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "import shutil\n",
        "import glob as glob\n",
        "import albumentations\n",
        "from roboflow import Roboflow\n",
        "from albumentations.pytorch import ToTensorV2"
      ],
      "metadata": {
        "id": "aJ6p4haUHdhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cloning our own Github Repository\n",
        "Here we are cloning my git repository. This will import in the environment all the code and funcions to train the network and process the data."
      ],
      "metadata": {
        "id": "Yn3tTBoOjpcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/EdoGiordani99/Traffic-Light-Detector.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIXCjG8qg_I-",
        "outputId": "ddbac53a-8d4d-456f-cbd2-200357e8334f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Traffic-Light-Detector' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reorganizing the Folders\n",
        "(just to make things more clear)"
      ],
      "metadata": {
        "id": "WjGpyVFmrReG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialiting the output directory\n",
        "!mkdir outputs\n",
        "\n",
        "# Transfering .py files to main directory\n",
        "for filename in os.listdir('Traffic-Light-Detector'):\n",
        "    if filename != '.git' and filename != 'README.md': \n",
        "        old = 'Traffic-Light-Detector/'+filename\n",
        "        new = filename\n",
        "\n",
        "        shutil.move(old, new)\n",
        "\n",
        "# Remooving the github repository folder\n",
        "!rm -rf Traffic-Light-Detector"
      ],
      "metadata": {
        "id": "34ZUzHr7iXfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59973d0d-10e5-47a7-99ea-d0dcee0d2273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘outputs’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the Dataset\n",
        "The dataset was manually collected by myself: I shooted some photos with an high resolution camera of traffic lights all over the city. The images were then processed with the help of the **RoboFlow** framework. This made the annotation and labeling process really easy and fast. \n",
        "\n",
        "Here we are simply downloading the annotated dataset from my Roboflow Account"
      ],
      "metadata": {
        "id": "4cyjn9VKqy6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = Roboflow(api_key=\"cJ4ImlUPhvj3p4yBOaJj\")\n",
        "project = rf.workspace(\"traffic-light-detector-bnryi\").project(\"traffic-light-detector-o3g9q\")\n",
        "dataset = project.version(2).download(\"voc\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY82msI4Hb6d",
        "outputId": "b55a16f9-a28d-4669-9d07-8af358d9ac81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in Traffic-Light-Detector-2 to voc: 100% [372820360 / 372820360] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to Traffic-Light-Detector-2 in voc:: 100%|██████████| 413/413 [00:14<00:00, 29.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.rename('Traffic-Light-Detector-2', 'data')"
      ],
      "metadata": {
        "id": "GJtgRIFapA11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "Finally, we get to the training. All the hyper-parameters can be set into the *config.py* file.\n",
        "\n",
        "\n",
        "Unfortunately, due to Colab limitations, I didn't manage to carry on with very long training (over 50 epochs). Epochs could be increased by reducing image size: in this way training will be faster since we have less parameters to learn."
      ],
      "metadata": {
        "id": "Lr8SUQ5Lq3Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run only if a previous training was done\n",
        "!rm outputs/valid_history\n",
        "!rm outputs/train_history"
      ],
      "metadata": {
        "id": "Kln2Rbf5zu_u",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf7c635-ddb2-4d97-e202-1c1545b16ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'outputs/valid_history': No such file or directory\n",
            "rm: cannot remove 'outputs/train_history': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "id": "V7Xi_zU_-xct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "326c838d-e26d-458a-a4a2-99c32fe36d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Number of training samples: 143\n",
            "Number of validation samples: 35\n",
            "\n",
            "Setting the Adam optimizer\n",
            "\n",
            "EPOCH 1 of 50\n",
            "Loss: 0.4117: 100% 18/18 [00:57<00:00,  3.17s/it]\n",
            "Validating\n",
            "Loss: 0.3998: 100% 5/5 [00:18<00:00,  3.60s/it]\n",
            "Epoch #1 train loss: 1.560\n",
            "Epoch #1 validation loss: 0.586\n",
            "Took 1.254 minutes for epoch 0\n",
            "\n",
            "Best validation loss: 0.5863295555114746\n",
            "\n",
            "Saving best model for epoch: 1\n",
            "\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 2 of 50\n",
            "Loss: 0.3596: 100% 18/18 [01:03<00:00,  3.51s/it]\n",
            "Validating\n",
            "Loss: 0.4275: 100% 5/5 [00:19<00:00,  3.94s/it]\n",
            "Epoch #2 train loss: 0.656\n",
            "Epoch #2 validation loss: 0.625\n",
            "Took 1.388 minutes for epoch 1\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 3 of 50\n",
            "Loss: 0.4022: 100% 18/18 [00:58<00:00,  3.24s/it]\n",
            "Validating\n",
            "Loss: 0.4082: 100% 5/5 [00:18<00:00,  3.74s/it]\n",
            "Epoch #3 train loss: 0.645\n",
            "Epoch #3 validation loss: 0.630\n",
            "Took 1.289 minutes for epoch 2\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 4 of 50\n",
            "Loss: 0.4038: 100% 18/18 [00:57<00:00,  3.19s/it]\n",
            "Validating\n",
            "Loss: 0.3471: 100% 5/5 [00:18<00:00,  3.74s/it]\n",
            "Epoch #4 train loss: 0.615\n",
            "Epoch #4 validation loss: 0.599\n",
            "Took 1.272 minutes for epoch 3\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 5 of 50\n",
            "Loss: 0.4412: 100% 18/18 [00:58<00:00,  3.23s/it]\n",
            "Validating\n",
            "Loss: 0.3486: 100% 5/5 [00:18<00:00,  3.79s/it]\n",
            "Epoch #5 train loss: 0.583\n",
            "Epoch #5 validation loss: 0.565\n",
            "Took 1.289 minutes for epoch 4\n",
            "\n",
            "Best validation loss: 0.565247905254364\n",
            "\n",
            "Saving best model for epoch: 5\n",
            "\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 6 of 50\n",
            "Loss: 0.7848: 100% 18/18 [00:59<00:00,  3.30s/it]\n",
            "Validating\n",
            "Loss: 0.3859: 100% 5/5 [00:18<00:00,  3.74s/it]\n",
            "Epoch #6 train loss: 0.580\n",
            "Epoch #6 validation loss: 0.617\n",
            "Took 1.307 minutes for epoch 5\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 7 of 50\n",
            "Loss: 0.5200: 100% 18/18 [00:59<00:00,  3.32s/it]\n",
            "Validating\n",
            "Loss: 0.3877: 100% 5/5 [00:18<00:00,  3.73s/it]\n",
            "Epoch #7 train loss: 0.571\n",
            "Epoch #7 validation loss: 0.578\n",
            "Took 1.312 minutes for epoch 6\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 8 of 50\n",
            "Loss: 0.4497: 100% 18/18 [00:57<00:00,  3.17s/it]\n",
            "Validating\n",
            "Loss: 0.3386: 100% 5/5 [00:20<00:00,  4.02s/it]\n",
            "Epoch #8 train loss: 0.556\n",
            "Epoch #8 validation loss: 0.594\n",
            "Took 1.292 minutes for epoch 7\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 9 of 50\n",
            "Loss: 0.5339: 100% 18/18 [00:57<00:00,  3.21s/it]\n",
            "Validating\n",
            "Loss: 0.3442: 100% 5/5 [00:18<00:00,  3.73s/it]\n",
            "Epoch #9 train loss: 0.516\n",
            "Epoch #9 validation loss: 0.598\n",
            "Took 1.278 minutes for epoch 8\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 10 of 50\n",
            "Loss: 0.3457: 100% 18/18 [00:59<00:00,  3.32s/it]\n",
            "Validating\n",
            "Loss: 0.4192: 100% 5/5 [00:18<00:00,  3.69s/it]\n",
            "Epoch #10 train loss: 0.520\n",
            "Epoch #10 validation loss: 0.613\n",
            "Took 1.308 minutes for epoch 9\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 11 of 50\n",
            "Loss: 0.3445: 100% 18/18 [00:58<00:00,  3.23s/it]\n",
            "Validating\n",
            "Loss: 0.2530: 100% 5/5 [00:18<00:00,  3.73s/it]\n",
            "Epoch #11 train loss: 0.496\n",
            "Epoch #11 validation loss: 0.447\n",
            "Took 1.286 minutes for epoch 10\n",
            "\n",
            "Best validation loss: 0.4467115938663483\n",
            "\n",
            "Saving best model for epoch: 11\n",
            "\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 12 of 50\n",
            "Loss: 0.4394: 100% 18/18 [01:02<00:00,  3.46s/it]\n",
            "Validating\n",
            "Loss: 0.2944: 100% 5/5 [00:18<00:00,  3.79s/it]\n",
            "Epoch #12 train loss: 0.464\n",
            "Epoch #12 validation loss: 0.456\n",
            "Took 1.359 minutes for epoch 11\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 13 of 50\n",
            "Loss: 0.5840: 100% 18/18 [00:58<00:00,  3.22s/it]\n",
            "Validating\n",
            "Loss: 0.2977: 100% 5/5 [00:18<00:00,  3.72s/it]\n",
            "Epoch #13 train loss: 0.474\n",
            "Epoch #13 validation loss: 0.433\n",
            "Took 1.282 minutes for epoch 12\n",
            "\n",
            "Best validation loss: 0.4328391313552856\n",
            "\n",
            "Saving best model for epoch: 13\n",
            "\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 14 of 50\n",
            "Loss: 0.6029: 100% 18/18 [01:00<00:00,  3.37s/it]\n",
            "Validating\n",
            "Loss: 0.3227: 100% 5/5 [00:19<00:00,  3.85s/it]\n",
            "Epoch #14 train loss: 0.467\n",
            "Epoch #14 validation loss: 0.475\n",
            "Took 1.336 minutes for epoch 13\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 15 of 50\n",
            "Loss: 0.5258: 100% 18/18 [00:58<00:00,  3.27s/it]\n",
            "Validating\n",
            "Loss: 0.3988: 100% 5/5 [00:18<00:00,  3.72s/it]\n",
            "Epoch #15 train loss: 0.491\n",
            "Epoch #15 validation loss: 0.552\n",
            "Took 1.296 minutes for epoch 14\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 16 of 50\n",
            "Loss: 0.5737: 100% 18/18 [00:57<00:00,  3.20s/it]\n",
            "Validating\n",
            "Loss: 0.3092: 100% 5/5 [00:20<00:00,  4.05s/it]\n",
            "Epoch #16 train loss: 0.515\n",
            "Epoch #16 validation loss: 0.492\n",
            "Took 1.304 minutes for epoch 15\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 17 of 50\n",
            "Loss: 0.3328: 100% 18/18 [00:58<00:00,  3.22s/it]\n",
            "Validating\n",
            "Loss: 0.3567: 100% 5/5 [00:18<00:00,  3.72s/it]\n",
            "Epoch #17 train loss: 0.526\n",
            "Epoch #17 validation loss: 0.573\n",
            "Took 1.282 minutes for epoch 16\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 18 of 50\n",
            "Loss: 0.6685: 100% 18/18 [00:59<00:00,  3.33s/it]\n",
            "Validating\n",
            "Loss: 0.3596: 100% 5/5 [00:18<00:00,  3.66s/it]\n",
            "Epoch #18 train loss: 0.525\n",
            "Epoch #18 validation loss: 0.544\n",
            "Took 1.308 minutes for epoch 17\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 19 of 50\n",
            "Loss: 0.5804: 100% 18/18 [00:58<00:00,  3.25s/it]\n",
            "Validating\n",
            "Loss: 0.4221: 100% 5/5 [00:18<00:00,  3.69s/it]\n",
            "Epoch #19 train loss: 0.492\n",
            "Epoch #19 validation loss: 0.552\n",
            "Took 1.288 minutes for epoch 18\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 20 of 50\n",
            "Loss: 0.4676: 100% 18/18 [00:59<00:00,  3.31s/it]\n",
            "Validating\n",
            "Loss: 0.4131: 100% 5/5 [00:18<00:00,  3.66s/it]\n",
            "Epoch #20 train loss: 0.529\n",
            "Epoch #20 validation loss: 0.612\n",
            "Took 1.302 minutes for epoch 19\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 21 of 50\n",
            "Loss: 0.8146: 100% 18/18 [00:57<00:00,  3.18s/it]\n",
            "Validating\n",
            "Loss: 0.4199: 100% 5/5 [00:18<00:00,  3.65s/it]\n",
            "Epoch #21 train loss: 0.546\n",
            "Epoch #21 validation loss: 0.616\n",
            "Took 1.265 minutes for epoch 20\n",
            "LOSS PLOTS HAVE BEEN SAVED!\n",
            "\n",
            "EPOCH 22 of 50\n",
            "Loss: 0.4770:  22% 4/18 [00:25<01:28,  6.32s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 174, in <module>\n",
            "    train_loss = train(train_dataloader, model)\n",
            "  File \"train.py\", line 43, in train\n",
            "    for i, data in enumerate(prog_bar):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tqdm/std.py\", line 1195, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 652, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1330, in _next_data\n",
            "    idx, data = self._get_data()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1296, in _get_data\n",
            "    success, data = self._try_get_data()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1134, in _try_get_data\n",
            "    data = self._data_queue.get(timeout=timeout)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 104, in get\n",
            "    if not self._poll(timeout):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 257, in poll\n",
            "    return self._poll(timeout)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 414, in _poll\n",
            "    r = wait([self], timeout)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 921, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.7/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save the model and histories into the drive"
      ],
      "metadata": {
        "id": "jiCGdijNn4PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copying histories and trained models to make comparisons\n",
        "!cp outputs/train_history trainings/7/\n",
        "!cp outputs/valid_history trainings/7/\n",
        "!cp outputs/best_model.pth trainings/7/\n",
        "!cp outputs/last_model.pth trainings/7/"
      ],
      "metadata": {
        "id": "mPUOIhjLClJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-val loss Preview"
      ],
      "metadata": {
        "id": "uMMBHbfVoBeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('outputs/train_history', 'rb') as f:\n",
        "    train_history = pickle.load(f)\n",
        "\n",
        "with open('outputs/valid_history', 'rb') as f:\n",
        "    valid_history = pickle.load(f)"
      ],
      "metadata": {
        "id": "SF54vzjLxeWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "wtgcFgul5M11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "\n",
        "x = np.linspace(0, len(train_history), len(train_history))\n",
        "train_line = ax.plot(x, train_history);\n",
        "valid_line = ax.plot(x, valid_history);\n",
        "ax.legend(['train loss', 'valid loss'])\n",
        " \n",
        "ax.ylim([1, 0])"
      ],
      "metadata": {
        "id": "TeT8dHpG4zAt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}